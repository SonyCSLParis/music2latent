{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# music2latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import IPython\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Inferencer model. You can specify the model checkpoint you want to load by using the load_path_inference argument, otherwise the default one will be loaded (specified in hparams_inference.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music2latent import EncoderDecoder\n",
    "\n",
    "encdec = EncoderDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an audio file for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = librosa.example('trumpet')\n",
    "\n",
    "wv, sr = librosa.load(audio_path, sr=44100)\n",
    "\n",
    "IPython.display.display(IPython.display.Audio(wv, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode an audio sample into latents, you need to provide the waveform as input, with shape [audio_channels, waveform_samples] or simply [waveform_samples,]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, sr = librosa.load(audio_path, sr=44100)\n",
    "print(f'waveform samples: {wv.shape}')\n",
    "\n",
    "latent = encdec.encode(wv)\n",
    "print(f'Shape of latents: {latent.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also process a batch of waveforms. Just use a numpy array with shape [batch_size, waveform_samples] as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, sr = librosa.load(audio_path, sr=44100)\n",
    "\n",
    "# create a batch of waveforms\n",
    "wv_batched = np.stack([wv]*3, axis=0)\n",
    "print(f'batch of waveforms shape: {wv_batched.shape}')\n",
    "\n",
    "latent_batched = encdec.encode(wv_batched)\n",
    "print(f'Shape of batched latents: {latent_batched.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode latent embeddings back to waveform, be sure to have latents with shape [batch_size/audio_channels, latent_dim, latent_length]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_rec = encdec.decode(latent)\n",
    "print(f'Shape of decoded waveform: {wv_rec.shape}')\n",
    "\n",
    "print(wv.shape, wv_rec.shape)\n",
    "\n",
    "print('Original')\n",
    "IPython.display.display(IPython.display.Audio(wv, rate=sr))\n",
    "print('Reconstructed')\n",
    "IPython.display.display(IPython.display.Audio(wv_rec.squeeze().cpu().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify how many denoising steps to perform (default is 1). However, we do not notice any improvements in audio quality by increasing the denoise_steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_rec = encdec.decode(latent, denoising_steps=3)\n",
    "print(f'Shape of decoded waveform: {wv_rec.shape}')\n",
    "\n",
    "print('Original')\n",
    "IPython.display.display(IPython.display.Audio(wv, rate=sr))\n",
    "print('Reconstructed')\n",
    "IPython.display.display(IPython.display.Audio(wv_rec.squeeze().cpu().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping GPU memory under control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder model needs plenty of memory to encode and decode samples.\n",
    "We offer a way to keep the memory usage under control.\n",
    "\n",
    "You can specify both the __max_batch_size__ and __max_waveform_length__ to use for encoding or decoding samples.\n",
    "\n",
    "If not specified, the defalt values are the ones in hparams_inference.py (__max_batch_size__=1, __max_waveform_length__=44100*10)\n",
    "\n",
    "If the waveform sample to encode or to reconstruct is longer than __max_waveform_length__, the spectrogram representation will be split into multiple samples, processed sequentially, and then concatenated back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, sr = librosa.load(audio_path, sr=44100)\n",
    "print(f'waveform samples: {wv.shape}')\n",
    "\n",
    "# split spectrogram into 1 second chunks, process each chunk separately, concatenate the results\n",
    "# much lower memory usage\n",
    "latent = encdec.encode(wv, max_waveform_length=44100*1)\n",
    "print(f'Shape of latents: {latent.shape}')\n",
    "\n",
    "wv_rec = encdec.decode(latent, max_waveform_length=44100*1)\n",
    "print(f'Shape of decoded waveform: {wv_rec.shape}')\n",
    "\n",
    "print('Original')\n",
    "IPython.display.display(IPython.display.Audio(wv, rate=sr))\n",
    "print('Reconstructed')\n",
    "IPython.display.display(IPython.display.Audio(wv_rec.squeeze().cpu().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to encode/decode batches of samples in parallel you can increase the __max_batch_size__ argument until you reach your maximum memory budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, sr = librosa.load(audio_path, sr=44100)\n",
    "\n",
    "# create a batch of waveforms\n",
    "wv_batched = np.stack([wv]*3, axis=0)\n",
    "print(f'batch of waveforms shape: {wv_batched.shape}')\n",
    "\n",
    "latent_batched = encdec.encode(wv_batched, max_batch_size=3)\n",
    "print(f'Shape of batched latents: {latent_batched.shape}')\n",
    "\n",
    "wv_rec = encdec.decode(latent, max_batch_size=3)\n",
    "print(f'Shape of decoded waveform: {wv_rec.shape}')\n",
    "\n",
    "print('Original')\n",
    "IPython.display.display(IPython.display.Audio(wv, rate=sr))\n",
    "print('Reconstructed')\n",
    "IPython.display.display(IPython.display.Audio(wv_rec[0].squeeze().cpu().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep in Mind:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the latents for generation tasks using diffusion-type models, make sure to properly normalize the latents according to the chosen diffusion framework. The latents extracted with this library DO NOT have unit standard deviation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onestep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
